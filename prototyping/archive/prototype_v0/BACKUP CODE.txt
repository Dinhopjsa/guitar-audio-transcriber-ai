CNN MODEL 1

    class CNNClassifier(nn.Module):
        """Simple 2-layer CNN + global avg pool → linear."""
        def __init__(self, num_classes):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(1, 16, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2,2),

                nn.Conv2d(16, 32, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2,2),
            )
            self.classifier = nn.Sequential(
                nn.AdaptiveAvgPool2d((1,1)),
                nn.Flatten(),
                nn.Linear(32, num_classes)
            )

        def forward(self, x):
            x = self.features(x)
            return self.classifier(x)

        def train_model(self, train_loader, test_loader=None,
                        epochs=50, lr=1e-3, device=None):
            device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.to(device)
            opt   = optim.Adam(self.parameters(), lr=lr)
            lossf = nn.CrossEntropyLoss()

            for ep in range(1, epochs+1):
                self.train()
                running_loss, correct = 0.0, 0
                for Xb, yb in train_loader:
                    Xb, yb = Xb.to(device), yb.to(device)
                    logits = self(Xb)
                    loss   = lossf(logits, yb)
                    opt.zero_grad()
                    loss.backward()
                    opt.step()
                    running_loss += loss.item() * Xb.size(0)
                    correct      += (logits.argmax(1) == yb).sum().item()
                train_loss = running_loss / len(train_loader.dataset)
                train_acc  = correct      / len(train_loader.dataset)

                msg = f"Epoch {ep}/{epochs} — loss: {train_loss:.3f} — acc: {train_acc:.2%}"
                if test_loader:
                    val_loss, val_acc = self.evaluate(test_loader, device)
                    msg += f" — val_loss: {val_loss:.3f} — val_acc: {val_acc:.2%}"
                print(msg)

        def evaluate(self, loader, device=None):
            device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.eval()
            lossf = nn.CrossEntropyLoss(reduction="sum")
            total_loss, correct = 0.0, 0
            with torch.no_grad():
                for Xb, yb in loader:
                    Xb, yb = Xb.to(device), yb.to(device)
                    logits = self(Xb)
                    total_loss += lossf(logits, yb).item()
                    correct    += (logits.argmax(1) == yb).sum().item()
            avg_loss = total_loss / len(loader.dataset)
            acc      = correct    / len(loader.dataset)
            return avg_loss, acc


    # CNN Main:
    def make_dataloaders(dataset_name="Kaggle_Electric_Open_Notes",
                        batch_size=32, 
                        test_size=0.2, 
                        seed=42):
        loader = AudioDatasetLoader(dataset_name=validate_dataset_name(dataset_name), seed=seed)
        paths, labels, le = loader.load_files_and_labels()
        from sklearn.model_selection import train_test_split
        Xtr, Xte, ytr, yte = train_test_split(
            paths, labels, test_size=test_size,
            random_state=seed, stratify=labels
        )
        train_dl = DataLoader(MelNoteDataset(Xtr, ytr), batch_size=batch_size, shuffle=True)
        test_dl  = DataLoader(MelNoteDataset(Xte, yte), batch_size=batch_size)
        return train_dl, test_dl, le


    train_dl, test_dl, label_encoder = make_dataloaders(dataset_name=AVAILABLE_DATASETS[1])
    model = CNNClassifier(num_classes=len(label_encoder.classes_))
    model.train_model(train_loader=train_dl, test_loader=test_dl, epochs=50, lr=0.001)
    test_loss, test_acc = model.evaluate(test_dl)
    print(f"\nFinal Test — loss: {test_loss:.3f} — acc: {test_acc:.2%}")












CNN MODEL 2

class MelNoteDataset(Dataset):
    """Loads mel-spectrograms on the fly from file paths + labels."""
    def __init__(self, file_paths, labels,
                n_mels=128, sr=44100, hop_length=256, duration=None):
        self.paths      = file_paths
        self.labels     = labels
        self.n_mels     = n_mels
        self.sr         = sr
        self.hop        = hop_length
        self.duration   = duration      # used for different size samples
        if duration is not None:
            self.fixed_len = int(self.sr * duration)
        else:
            self.fixed_len = None

        #print(f"Number of samples: {len(file_paths)}")

    def __len__(self):
        return len(self.paths)

    def _fix_length(self, y: np.ndarray) -> np.ndarray:
        if self.fixed_len is None:
            return y

        L = len(y)
        N = self.fixed_len

        if L > N:
            # trim
            return y[:N]
        elif L < N:
            # pad with zeros
            pad_width = N - L
            return np.pad(y, (0, pad_width), mode="constant")
        else:
            # exactly the right length
            return y

    def __getitem__(self, idx):
        path  = self.paths[idx]
        label = self.labels[idx]
        # 1) load audio
        y, _ = librosa.load(path, sr=self.sr, mono=True)

        # 1.1) pad if short
        y = self._fix_length(y) # DOUBLE CHECK
        
        # 2) mel-spectrogram
        S = librosa.feature.melspectrogram(
            y=y,
            sr=self.sr,
            n_mels=self.n_mels,
            hop_length=self.hop,
            fmax=self.sr // 2,
        )
        S_db = librosa.power_to_db(S, ref=np.max)

        # 3) normalize to [0,1]
        S_norm = (S_db + 80) / 80

        # 4) to tensor: (C=1, H=n_mels, W=frames)
        mel = torch.tensor(S_norm, dtype=torch.float32).unsqueeze(0)
        return mel, label
    
    def print_paths(self):
        for path in self.paths:
            print(path)


class CNNClassifier(nn.Module):
    def __init__(self, num_classes, lr=1e-3, weight_decay=0.0, dataset=AVAILABLE_DATASETS[0], duration=0.5):
        super().__init__()
        self.lr = lr
        self.wd = weight_decay
        self.dataset=dataset
        self.duration=duration
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2,2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(32, num_classes)
        )
        self.loss_history = []
        self.acc_history   = []

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)

    def fit(self, train_dl, val_dl=None, epochs=50, device=None, log_every=5):
        device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(device)
        opt   = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)
        lossf = nn.CrossEntropyLoss()

        for ep in range(1, epochs+1):
            self.train()
            running_loss, correct = 0.0, 0
            for Xb, yb in train_dl:
                Xb, yb = Xb.to(device), yb.to(device)
                logits = self(Xb)
                loss   = lossf(logits, yb)
                opt.zero_grad()
                loss.backward()
                opt.step()
                running_loss += loss.item() * Xb.size(0)
                correct      += (logits.argmax(1) == yb).sum().item()

            train_loss = running_loss / len(train_dl.dataset)
            train_acc  = correct      / len(train_dl.dataset)
            self.loss_history.append(train_loss)
            self.acc_history.append(train_acc)

            if ep % log_every == 0 or ep == 1:
                msg = f"[Epoch {ep}/{epochs}] loss: {train_loss:.3f} acc: {train_acc:.2%}"
                if val_dl:
                    val_loss, val_acc = self.evaluate(val_dl, device)
                    msg += f" — val_loss: {val_loss:.3f} val_acc: {val_acc:.2%}"
                print(msg)


    def evaluate(self, dl, device=None):
        device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.eval()
        lossf = nn.CrossEntropyLoss(reduction="sum")
        total_loss, correct = 0.0, 0
        with torch.no_grad():
            for Xb, yb in dl:
                Xb, yb = Xb.to(device), yb.to(device)
                logits = self(Xb)
                total_loss += lossf(logits, yb).item()
                correct    += (logits.argmax(1) == yb).sum().item()
        avg_loss = total_loss / len(dl.dataset)
        acc      = correct    / len(dl.dataset)
        return avg_loss, acc
    

    def train_cnn(self, batch_size=64, test_size=0.2, epochs=50, seed=42, plot_loss=False):
        # ——— Prepare data ———
        loader = AudioDatasetLoader(dataset_name=self.dataset)
        paths, labels, label_encoder = loader.load_files_and_labels()
        Xtr, Xte, ytr, yte = train_test_split(
            paths, labels,
            test_size=test_size,
            random_state=seed,
            stratify=labels
        )
        train_dl = DataLoader(MelNoteDataset(Xtr, ytr, duration=self.duration), batch_size=batch_size, shuffle=True)
        test_dl  = DataLoader(MelNoteDataset(Xte, yte, duration=self.duration), batch_size=batch_size)
        
        # ——— Train & evaluate ———
        #model = CNNClassifier(num_classes=len(label_encoder.classes_))
        self.fit(train_dl, val_dl=test_dl, epochs=epochs, device=None, log_every=5)
        test_loss, test_acc = self.evaluate(test_dl)
        print(f"\nFinal Test — loss: {test_loss:.3f} — acc: {test_acc:.2%}")

        # ——— Plot learning curves ———
        if plot_loss:
            plot_data(
                {"train_loss": self.loss_history, "train_acc": self.acc_history},
                xlabel="Epoch",
                ylabel="Value",
                title="CNN Training Curves",
            )
            
        #return model # Warning: was initially training a seperate model, not itself
    

    def forward_clips(self, clips, device=None, sr=44100, n_mels=64, hop_length=512): # Reminder: possibly add sr in __init__ (self.sr)
        # Used in transcribe to pass audio segments trimmed with OnsetSegmenter into forward and return notes (y)
        device = device or next(self.parameters()).device

        fixed_len = (sr * self.duration)

        specs = []
        for clip in clips:
            # 0) enforce exact length via truncate or pad
            if len(clip) < fixed_len:
                # pad at end
                clip = np.pad(clip, (0, int(fixed_len - len(clip))), mode="constant")
            elif len(clip) > fixed_len:
                # truncate
                clip = clip[:fixed_len]

            # 1) mel-spectrogram
            S = librosa.feature.melspectrogram(
                y=clip,
                sr=sr,
                n_mels=n_mels,
                hop_length=hop_length
            )
            S_db   = librosa.power_to_db(S, ref=np.max)
            S_norm = (S_db + 80) / 80
            specs.append(S_norm)

        # 2) stack into tensor (N,1,H,W)
        array = np.stack(specs, axis=0)
        tensor = torch.tensor(array, dtype=torch.float32).unsqueeze(1).to(device)

        # 3) forward batch
        with torch.no_grad():
            logits = self(tensor)              # calls forward()
            idxs   = logits.argmax(dim=1).cpu().numpy()

        # 4) map back to strings
        notes = self.label_encoder.inverse_transform(idxs)
        return notes        


def train_model(batch_size=64, test_size=0.2, epochs=50, seed=42, plot_loss=False, dataset=AVAILABLE_DATASETS[0], duration=0.5):
    # ——— Prepare data ———
    loader = AudioDatasetLoader(dataset_name=dataset)
    paths, labels, label_encoder = loader.load_files_and_labels()
    Xtr, Xte, ytr, yte = train_test_split(
        paths, labels,
        test_size=test_size,
        random_state=seed,
        stratify=labels
    )
    train_dl = DataLoader(MelNoteDataset(Xtr, ytr, duration=duration), batch_size=batch_size, shuffle=True)
    test_dl  = DataLoader(MelNoteDataset(Xte, yte, duration=duration), batch_size=batch_size)
    
    # ——— Train & evaluate ———
    model = CNNClassifier(num_classes=len(label_encoder.classes_))
    model.fit(train_dl, val_dl=test_dl, epochs=epochs, device=None, log_every=5)
    test_loss, test_acc = model.evaluate(test_dl)
    print(f"\nFinal Test — loss: {test_loss:.3f} — acc: {test_acc:.2%}")

    # ——— Plot learning curves ———
    if plot_loss:
        plot_data(
            {"train_loss": model.loss_history, "train_acc": model.acc_history},
            xlabel="Epoch",
            ylabel="Value",
            title="CNN Training Curves",
        )
        
    return model